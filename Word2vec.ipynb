{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "V8MYNbuF2dOK"
   },
   "source": [
    "# **Import Libraries**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "hHplmqAFYOJt",
    "outputId": "18c7e5e9-6a53-4c4e-82c3-af7ce778a50f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: lime in c:\\users\\win10\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (0.2.0.1)Note: you may need to restart the kernel to use updated packages.\n",
      "Requirement already satisfied: matplotlib in c:\\users\\win10\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from lime) (3.5.1)\n",
      "Requirement already satisfied: scikit-learn>=0.18 in c:\\users\\win10\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from lime) (1.0.1)\n",
      "Requirement already satisfied: scipy in c:\\users\\win10\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from lime) (1.7.3)\n",
      "\n",
      "Requirement already satisfied: numpy in c:\\users\\win10\\appdata\\roaming\\python\\python39\\site-packages (from lime) (1.22.2)\n",
      "Requirement already satisfied: scikit-image>=0.12 in c:\\users\\win10\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from lime) (0.19.3)\n",
      "Requirement already satisfied: tqdm in c:\\users\\win10\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from lime) (4.62.3)\n",
      "Requirement already satisfied: pillow!=7.1.0,!=7.1.1,!=8.3.0,>=6.1.0 in c:\\users\\win10\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from scikit-image>=0.12->lime) (9.0.0)\n",
      "Requirement already satisfied: tifffile>=2019.7.26 in c:\\users\\win10\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from scikit-image>=0.12->lime) (2022.10.10)\n",
      "Requirement already satisfied: PyWavelets>=1.1.1 in c:\\users\\win10\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from scikit-image>=0.12->lime) (1.4.1)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\win10\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from scikit-image>=0.12->lime) (21.2)\n",
      "Requirement already satisfied: networkx>=2.2 in c:\\users\\win10\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from scikit-image>=0.12->lime) (2.8.8)\n",
      "Requirement already satisfied: imageio>=2.4.1 in c:\\users\\win10\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from scikit-image>=0.12->lime) (2.22.4)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in c:\\users\\win10\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from scikit-learn>=0.18->lime) (3.0.0)\n",
      "Requirement already satisfied: joblib>=0.11 in c:\\users\\win10\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from scikit-learn>=0.18->lime) (1.1.0)\n",
      "Requirement already satisfied: pyparsing>=2.2.1 in c:\\users\\win10\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from matplotlib->lime) (2.4.7)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in c:\\users\\win10\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from matplotlib->lime) (1.3.1)\n",
      "Requirement already satisfied: cycler>=0.10 in c:\\users\\win10\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from matplotlib->lime) (0.10.0)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in c:\\users\\win10\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from matplotlib->lime) (2.8.2)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in c:\\users\\win10\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from matplotlib->lime) (4.29.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\win10\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from tqdm->lime) (0.4.4)\n",
      "Requirement already satisfied: six in c:\\users\\win10\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from cycler>=0.10->matplotlib->lime) (1.15.0)\n"
     ]
    }
   ],
   "source": [
    "pip install lime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "id": "ytEAvuLT2Nxj"
   },
   "outputs": [],
   "source": [
    "# for data reading\n",
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# for plotting the data\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# for preprocessing\n",
    "import re\n",
    "import nltk\n",
    "\n",
    "#for bag-of-words (a method to extract features from text documents)\n",
    "from sklearn import feature_extraction, model_selection, naive_bayes, pipeline, manifold, preprocessing\n",
    "\n",
    "# for explainer\n",
    "from lime import lime_text\n",
    "\n",
    "# for word embedding\n",
    "import gensim\n",
    "import gensim.downloader as gensim_api\n",
    "\n",
    "# for deep learning\n",
    "from tensorflow.keras import models, layers, preprocessing as kprocessing\n",
    "from tensorflow.keras import backend as K\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rvNC8yNZGrw9"
   },
   "source": [
    "# **Data reading**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "XIPz02-4tBMB",
    "outputId": "3360c4e9-9090-43d3-fed2-2be20ebe3d04"
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"../fyp/mbti_full_pull.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 206
    },
    "id": "HA8OuXa1iAHe",
    "outputId": "3a80daf9-fb17-44bd-d780-67fa28bf7e0f"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>author_flair_text</th>\n",
       "      <th>body</th>\n",
       "      <th>subreddit</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>INTJ</td>\n",
       "      <td>Knowing you're in INTJ is a tool for you to us...</td>\n",
       "      <td>intj</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>INTJ</td>\n",
       "      <td>You are truly an enlightened mastermind.</td>\n",
       "      <td>intj</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>INFJ, 26F</td>\n",
       "      <td>You should :) it will help if you have a down ...</td>\n",
       "      <td>infj</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>INTP</td>\n",
       "      <td>I watch a bit of everything (including hentai)...</td>\n",
       "      <td>INTP</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>INTJ</td>\n",
       "      <td>I don't know if I would count this as a pet pe...</td>\n",
       "      <td>intj</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  author_flair_text                                               body  \\\n",
       "0              INTJ  Knowing you're in INTJ is a tool for you to us...   \n",
       "1              INTJ           You are truly an enlightened mastermind.   \n",
       "2         INFJ, 26F  You should :) it will help if you have a down ...   \n",
       "3              INTP  I watch a bit of everything (including hentai)...   \n",
       "4              INTJ  I don't know if I would count this as a pet pe...   \n",
       "\n",
       "  subreddit  \n",
       "0      intj  \n",
       "1      intj  \n",
       "2      infj  \n",
       "3      INTP  \n",
       "4      intj  "
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "0vlz6dhR7aRr",
    "outputId": "85e9adb4-ea8f-4388-c544-0c8d8e869bbd"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1794016, 3)"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 175
    },
    "id": "rbt5CuU-8EOA",
    "outputId": "c9cb72c2-315d-4c97-fcaa-52d6f1416039"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>author_flair_text</th>\n",
       "      <th>body</th>\n",
       "      <th>subreddit</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>1794016</td>\n",
       "      <td>1793961</td>\n",
       "      <td>1794016</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>unique</th>\n",
       "      <td>8702</td>\n",
       "      <td>1746610</td>\n",
       "      <td>520</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>top</th>\n",
       "      <td>INTP</td>\n",
       "      <td>Yes.</td>\n",
       "      <td>INTP</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>freq</th>\n",
       "      <td>365646</td>\n",
       "      <td>677</td>\n",
       "      <td>419700</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       author_flair_text     body subreddit\n",
       "count            1794016  1793961   1794016\n",
       "unique              8702  1746610       520\n",
       "top                 INTP     Yes.      INTP\n",
       "freq              365646      677    419700"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.describe(include='O')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "yWAPLPay8U2_",
    "outputId": "d33969c6-6f03-4dce-8456-b94ae1e186eb"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "INTP            419700\n",
       "intj            296101\n",
       "mbti            253602\n",
       "entp            178379\n",
       "infj            164662\n",
       "                 ...  \n",
       "PourPainting         1\n",
       "Trophies             1\n",
       "BF_Hardline          1\n",
       "PKA                  1\n",
       "mylittlepony         1\n",
       "Name: subreddit, Length: 520, dtype: int64"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[\"subreddit\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ZEU3EboH8kBP",
    "outputId": "f09d5fe2-7669-4b11-fbbd-2787726b813b"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "INTP                                     365646\n",
       "INTJ                                     323224\n",
       "ENFP                                      88334\n",
       "ENTP                                      73481\n",
       "INFJ                                      69730\n",
       "                                          ...  \n",
       "[INTj-Ne 5w6] Ask me about my hobbies         1\n",
       "5w6 SP | ISTP                                 1\n",
       "M-20-ENTP                                     1\n",
       "INFP/Leo/Hufflepuff                           1\n",
       "35M INTP                                      1\n",
       "Name: author_flair_text, Length: 8702, dtype: int64"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[\"author_flair_text\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "AAHC-WxKIWM7",
    "outputId": "3750e235-ebd6-4a9e-af5d-f3bc2d99da82"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "author_flair_text     0\n",
       "body                 55\n",
       "subreddit             0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# find # of missing value\n",
    "df.isnull().sum()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "n5NrqB_t678P",
    "outputId": "161539e9-d605-41ae-c0cd-e269a77f1dab"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "author_flair_text    0\n",
       "body                 0\n",
       "subreddit            0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# remove missing values\n",
    "df = df.dropna().reset_index(drop=True)\n",
    "df.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "TuIW0f_6-sDu",
    "outputId": "4754f251-fd40-4263-d4cb-0336746da5db"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<bound method NDFrame.head of                  y                                               text  \\\n",
       "0             INTJ  Knowing you're in INTJ is a tool for you to us...   \n",
       "1             INTJ           You are truly an enlightened mastermind.   \n",
       "2        INFJ, 26F  You should :) it will help if you have a down ...   \n",
       "3             INTP  I watch a bit of everything (including hentai)...   \n",
       "4             INTJ  I don't know if I would count this as a pet pe...   \n",
       "...            ...                                                ...   \n",
       "1793956       INTP                                Remind me! 40 hours   \n",
       "1793957       INTJ  We are seeing the start of a process that is g...   \n",
       "1793958       INTJ  [NSFDL. I cry laughing at this every single ti...   \n",
       "1793959       INFJ                                         Ravenclaw!   \n",
       "1793960       INTP  I struggle massive with focusing, I would love...   \n",
       "\n",
       "        subreddit  \n",
       "0            intj  \n",
       "1            intj  \n",
       "2            infj  \n",
       "3            INTP  \n",
       "4            intj  \n",
       "...           ...  \n",
       "1793956      INTP  \n",
       "1793957      intj  \n",
       "1793958      intj  \n",
       "1793959      infj  \n",
       "1793960      INTP  \n",
       "\n",
       "[1793961 rows x 3 columns]>"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# rename column that will be used\n",
    "df = df.rename(columns={\"author_flair_text\" : \"y\" , \"body\" : \"text\"})\n",
    "df.head"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "id": "AD2qnA_UoUZD"
   },
   "outputs": [],
   "source": [
    "# drop the column that will not be used\n",
    "df.drop(\"subreddit\", inplace=True, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "w5LLfOraokm5",
    "outputId": "f8796f45-f1ea-41c2-98ec-1ad1c72aa38f"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<bound method NDFrame.head of                  y                                               text\n",
       "0             INTJ  Knowing you're in INTJ is a tool for you to us...\n",
       "1             INTJ           You are truly an enlightened mastermind.\n",
       "2        INFJ, 26F  You should :) it will help if you have a down ...\n",
       "3             INTP  I watch a bit of everything (including hentai)...\n",
       "4             INTJ  I don't know if I would count this as a pet pe...\n",
       "...            ...                                                ...\n",
       "1793956       INTP                                Remind me! 40 hours\n",
       "1793957       INTJ  We are seeing the start of a process that is g...\n",
       "1793958       INTJ  [NSFDL. I cry laughing at this every single ti...\n",
       "1793959       INFJ                                         Ravenclaw!\n",
       "1793960       INTP  I struggle massive with focusing, I would love...\n",
       "\n",
       "[1793961 rows x 2 columns]>"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "j0rkrfVaoyND",
    "outputId": "ac5e2fa5-4910-4c80-ff7e-b95311b5c574"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "INTP                            366505\n",
       "INTJ                            323717\n",
       "ENFP                             88438\n",
       "ENTP                             80836\n",
       "INFJ                             71676\n",
       "                                 ...  \n",
       "AN ENTJ                              1\n",
       "INFJ |19 | F                         1\n",
       "INFJ | SURELY | ��                   1\n",
       "ENFP: IT'S GETTING COLD OUT.         1\n",
       "35M INTP                             1\n",
       "Name: y, Length: 8337, dtype: int64"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Cleaning for y column (converting to uppercase and remove noise)\n",
    "df[\"y\"] = df[\"y\"].str.upper()\n",
    "df[\"y\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "id": "xny_EHZ3wCh9"
   },
   "outputs": [],
   "source": [
    "# clear out noise\n",
    "import re\n",
    "\n",
    "pattern = \"[IE][SN][FT][PJ]\"\n",
    "\n",
    "matches = df[\"y\"].apply(lambda x: re.findall(\"[IE][SN][FT][PJ]\", x)).apply(lambda x: x[0] if x else None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "lcD99kSQYMs6",
    "outputId": "dc7c910c-b8aa-4acb-a9f3-0fe75f9a94d8"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<bound method NDFrame.head of             y                                               text\n",
       "0        INTJ  Knowing you're in INTJ is a tool for you to us...\n",
       "1        INTJ           You are truly an enlightened mastermind.\n",
       "2        INFJ  You should :) it will help if you have a down ...\n",
       "3        INTP  I watch a bit of everything (including hentai)...\n",
       "4        INTJ  I don't know if I would count this as a pet pe...\n",
       "...       ...                                                ...\n",
       "1793956  INTP                                Remind me! 40 hours\n",
       "1793957  INTJ  We are seeing the start of a process that is g...\n",
       "1793958  INTJ  [NSFDL. I cry laughing at this every single ti...\n",
       "1793959  INFJ                                         Ravenclaw!\n",
       "1793960  INTP  I struggle massive with focusing, I would love...\n",
       "\n",
       "[1793961 rows x 2 columns]>"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[\"y\"] = matches\n",
    "\n",
    "df.head"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "cve8zmjC-jD-",
    "outputId": "8aa44bc2-1695-42f4-938e-a090b2e602cd"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "INTP    474174\n",
       "INTJ    365450\n",
       "ENTP    223000\n",
       "INFJ    206966\n",
       "INFP    180164\n",
       "ENFP     98171\n",
       "ISTP     64965\n",
       "ESTP     53258\n",
       "ENTJ     51067\n",
       "ENFJ     21254\n",
       "ISTJ     18395\n",
       "ISFP     11430\n",
       "ISFJ      7911\n",
       "ESFP      7730\n",
       "ESTJ      7232\n",
       "ESFJ      2794\n",
       "Name: y, dtype: int64"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[\"y\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "id": "ST0HTGXa_nqW"
   },
   "outputs": [],
   "source": [
    "'''\n",
    "  Preprocess a string.\n",
    "  :parameter\n",
    "    :param text: string - name of column containing text\n",
    "    :param 1st_stopwords: list - list of stopwords to remove\n",
    "    :param flg_stemm: bool - whether stemming is to be applied\n",
    "    :param flg_lemm: bool - whether lemmentisation is to be applied\n",
    "  :return\n",
    "    cleaned text\n",
    "'''\n",
    "# Cleaning text column\n",
    "\n",
    "def utils_preprocess_text(text, flg_stemm=False, flg_lemm=True, first_stopwords=None):\n",
    "  # change to lowercase and remove punctuations and characters and then strip\n",
    "  text = re.sub(r'[^\\w\\s]', '', str(text).lower().strip())\n",
    "\n",
    "  # Tokenize (conver from string to list)\n",
    "  first_text = text.split()\n",
    "\n",
    "  # remove stopwords\n",
    "  if first_stopwords is not None:\n",
    "    first_text = [word for word in first_text if word not in first_stopwords]\n",
    "\n",
    "  # Stemming (removing -ing, -ly, -ed, .....)\n",
    "  if flg_stemm == True:\n",
    "    ps = nltk.stem.porter.PorterStemmer()\n",
    "    first_text = [ps.stem(word) for word in first_text]\n",
    "\n",
    "  # Lemmatisation (convert the word into root word)\n",
    "  if flg_lemm == True:\n",
    "    lem = nltk.stem.wordnet.WordNetLemmatizer()\n",
    "    first_text = [lem.lemmatize(word) for word in first_text]\n",
    "\n",
    "  # back to sring from list\n",
    "  text = \" \".join(first_text)\n",
    "  return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "92iwjMgoZxqy",
    "outputId": "bf55caed-80fa-4014-e7c2-506142d6d838"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\win10\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download(\"stopwords\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "vwH5CZGVZLE8",
    "outputId": "e498f55e-f54d-4c9e-f3cc-00a1e0577d4f"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['i',\n",
       " 'me',\n",
       " 'my',\n",
       " 'myself',\n",
       " 'we',\n",
       " 'our',\n",
       " 'ours',\n",
       " 'ourselves',\n",
       " 'you',\n",
       " \"you're\",\n",
       " \"you've\",\n",
       " \"you'll\",\n",
       " \"you'd\",\n",
       " 'your',\n",
       " 'yours',\n",
       " 'yourself',\n",
       " 'yourselves',\n",
       " 'he',\n",
       " 'him',\n",
       " 'his',\n",
       " 'himself',\n",
       " 'she',\n",
       " \"she's\",\n",
       " 'her',\n",
       " 'hers',\n",
       " 'herself',\n",
       " 'it',\n",
       " \"it's\",\n",
       " 'its',\n",
       " 'itself',\n",
       " 'they',\n",
       " 'them',\n",
       " 'their',\n",
       " 'theirs',\n",
       " 'themselves',\n",
       " 'what',\n",
       " 'which',\n",
       " 'who',\n",
       " 'whom',\n",
       " 'this',\n",
       " 'that',\n",
       " \"that'll\",\n",
       " 'these',\n",
       " 'those',\n",
       " 'am',\n",
       " 'is',\n",
       " 'are',\n",
       " 'was',\n",
       " 'were',\n",
       " 'be',\n",
       " 'been',\n",
       " 'being',\n",
       " 'have',\n",
       " 'has',\n",
       " 'had',\n",
       " 'having',\n",
       " 'do',\n",
       " 'does',\n",
       " 'did',\n",
       " 'doing',\n",
       " 'a',\n",
       " 'an',\n",
       " 'the',\n",
       " 'and',\n",
       " 'but',\n",
       " 'if',\n",
       " 'or',\n",
       " 'because',\n",
       " 'as',\n",
       " 'until',\n",
       " 'while',\n",
       " 'of',\n",
       " 'at',\n",
       " 'by',\n",
       " 'for',\n",
       " 'with',\n",
       " 'about',\n",
       " 'against',\n",
       " 'between',\n",
       " 'into',\n",
       " 'through',\n",
       " 'during',\n",
       " 'before',\n",
       " 'after',\n",
       " 'above',\n",
       " 'below',\n",
       " 'to',\n",
       " 'from',\n",
       " 'up',\n",
       " 'down',\n",
       " 'in',\n",
       " 'out',\n",
       " 'on',\n",
       " 'off',\n",
       " 'over',\n",
       " 'under',\n",
       " 'again',\n",
       " 'further',\n",
       " 'then',\n",
       " 'once',\n",
       " 'here',\n",
       " 'there',\n",
       " 'when',\n",
       " 'where',\n",
       " 'why',\n",
       " 'how',\n",
       " 'all',\n",
       " 'any',\n",
       " 'both',\n",
       " 'each',\n",
       " 'few',\n",
       " 'more',\n",
       " 'most',\n",
       " 'other',\n",
       " 'some',\n",
       " 'such',\n",
       " 'no',\n",
       " 'nor',\n",
       " 'not',\n",
       " 'only',\n",
       " 'own',\n",
       " 'same',\n",
       " 'so',\n",
       " 'than',\n",
       " 'too',\n",
       " 'very',\n",
       " 's',\n",
       " 't',\n",
       " 'can',\n",
       " 'will',\n",
       " 'just',\n",
       " 'don',\n",
       " \"don't\",\n",
       " 'should',\n",
       " \"should've\",\n",
       " 'now',\n",
       " 'd',\n",
       " 'll',\n",
       " 'm',\n",
       " 'o',\n",
       " 're',\n",
       " 've',\n",
       " 'y',\n",
       " 'ain',\n",
       " 'aren',\n",
       " \"aren't\",\n",
       " 'couldn',\n",
       " \"couldn't\",\n",
       " 'didn',\n",
       " \"didn't\",\n",
       " 'doesn',\n",
       " \"doesn't\",\n",
       " 'hadn',\n",
       " \"hadn't\",\n",
       " 'hasn',\n",
       " \"hasn't\",\n",
       " 'haven',\n",
       " \"haven't\",\n",
       " 'isn',\n",
       " \"isn't\",\n",
       " 'ma',\n",
       " 'mightn',\n",
       " \"mightn't\",\n",
       " 'mustn',\n",
       " \"mustn't\",\n",
       " 'needn',\n",
       " \"needn't\",\n",
       " 'shan',\n",
       " \"shan't\",\n",
       " 'shouldn',\n",
       " \"shouldn't\",\n",
       " 'wasn',\n",
       " \"wasn't\",\n",
       " 'weren',\n",
       " \"weren't\",\n",
       " 'won',\n",
       " \"won't\",\n",
       " 'wouldn',\n",
       " \"wouldn't\"]"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "first_stopwords = nltk.corpus.stopwords.words(\"english\")\n",
    "\n",
    "first_stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "tdXNCEzIWwSk",
    "outputId": "40d9ddbc-b857-4695-d3f3-75c10e5ebcec"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\win10\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     C:\\Users\\win10\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "id": "_4qir3k4Z0BQ"
   },
   "outputs": [],
   "source": [
    "df[\"text_clean\"] = df[\"text\"].apply(lambda x: utils_preprocess_text(x,flg_stemm = False, flg_lemm = True, first_stopwords = first_stopwords))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 285
    },
    "id": "T_-K36C6Yo0Y",
    "outputId": "c6867042-c5d2-4264-df8d-729d808af432",
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>y</th>\n",
       "      <th>text</th>\n",
       "      <th>text_clean</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>INTJ</td>\n",
       "      <td>Knowing you're in INTJ is a tool for you to us...</td>\n",
       "      <td>knowing youre intj tool use interaction people...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>INTJ</td>\n",
       "      <td>You are truly an enlightened mastermind.</td>\n",
       "      <td>truly enlightened mastermind</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>INFJ</td>\n",
       "      <td>You should :) it will help if you have a down ...</td>\n",
       "      <td>help moment hobby keep mind busy dont like loo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>INTP</td>\n",
       "      <td>I watch a bit of everything (including hentai)...</td>\n",
       "      <td>watch bit everything including hentai tend enj...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>INTJ</td>\n",
       "      <td>I don't know if I would count this as a pet pe...</td>\n",
       "      <td>dont know would count pet peeze something time...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      y                                               text  \\\n",
       "0  INTJ  Knowing you're in INTJ is a tool for you to us...   \n",
       "1  INTJ           You are truly an enlightened mastermind.   \n",
       "2  INFJ  You should :) it will help if you have a down ...   \n",
       "3  INTP  I watch a bit of everything (including hentai)...   \n",
       "4  INTJ  I don't know if I would count this as a pet pe...   \n",
       "\n",
       "                                          text_clean  \n",
       "0  knowing youre intj tool use interaction people...  \n",
       "1                       truly enlightened mastermind  \n",
       "2  help moment hobby keep mind busy dont like loo...  \n",
       "3  watch bit everything including hentai tend enj...  \n",
       "4  dont know would count pet peeze something time...  "
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "id": "_BwMnwlpXs5j"
   },
   "outputs": [],
   "source": [
    "# split dataset\n",
    "df_train, df_test = model_selection.train_test_split(df, test_size = 0.3)\n",
    "\n",
    "#get target\n",
    "y_train = df_train[\"y\"].values\n",
    "y_test = df_test[\"y\"].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "colab": {
     "background_save": true
    },
    "id": "CmbQGV5ZYmKy"
   },
   "outputs": [],
   "source": [
    "nlp = gensim_api.load(\"word2vec-google-news-300\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "colab": {
     "background_save": true
    },
    "id": "XextmYbaaLOE",
    "outputId": "27adaf8b-be92-4a64-a581-bc600c23c3a4"
   },
   "outputs": [],
   "source": [
    "corpus = df_train[\"text_clean\"]\n",
    "\n",
    "# create list of lists of unigrams\n",
    "first_corpus = []\n",
    "for string in corpus: \n",
    "  first_words = string.split()\n",
    "  first_grams = [\" \".join(first_words[i:i+1]) for i in range (0, len(first_words), 1)]\n",
    "  first_corpus.append(first_grams)\n",
    "\n",
    "# detect bigrams and trigrams\n",
    "bigrams_detector = gensim.models.phrases.Phrases(first_corpus, min_count=5, threshold=10)\n",
    "\n",
    "trigrams_detector = gensim.models.phrases.Phrases(bigrams_detector[first_corpus], min_count=5, threshold=10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "import multiprocessing\n",
    "\n",
    "from gensim.models import Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fit w2v\n",
    "nlp = gensim.models.word2vec.Word2Vec(first_corpus, vector_size = 300, window=8, min_count=1, sg=1, epochs=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "## tokenize text\n",
    "tokenizer = kprocessing.text.Tokenizer(lower=True, split=' ', \n",
    "                     oov_token=\"NaN\", \n",
    "                     filters='!\"#$%&()*+,-./:;<=>?@[\\\\]^_`{|}~\\t\\n')\n",
    "tokenizer.fit_on_texts(first_corpus)\n",
    "dic_vocabulary = tokenizer.word_index\n",
    "## create sequence\n",
    "first_text2seq= tokenizer.texts_to_sequences(first_corpus) \n",
    "## padding sequence\n",
    "X_train = kprocessing.sequence.pad_sequences(first_text2seq, \n",
    "                    maxlen=15, padding=\"post\", truncating=\"post\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "from:  im opposite tend get angry openly im comfortable people closest feel brunt rage people handle people closest already skin past wall dont need find way im selfish bastard way thats way little decided protect cant figure change didnt build wall around keep people theyre keep feeling | len: 46\n",
      "to:  [    3   543   181     8   792  2869     3   470     4  1437    16 16326\n",
      "  2796     4   800] | len: 15\n",
      "check:  im  -- idx in vocabulary --> 3\n",
      "vocabulary:  {'NaN': 1, 'like': 2, 'im': 3, 'people': 4, 'dont': 5} ... (padding element, 0)\n"
     ]
    }
   ],
   "source": [
    "i = 0\n",
    "\n",
    "## list of text: [\"I like this\", ...]\n",
    "len_txt = len(df_train[\"text_clean\"].iloc[i].split())\n",
    "print(\"from: \", df_train[\"text_clean\"].iloc[i], \"| len:\", len_txt)\n",
    "\n",
    "## sequence of token ids: [[1, 2, 3], ...]\n",
    "len_tokens = len(X_train[i])\n",
    "print(\"to: \", X_train[i], \"| len:\", len(X_train[i]))\n",
    "\n",
    "## vocabulary: {\"I\":1, \"like\":2, \"this\":3, ...}\n",
    "print(\"check: \", df_train[\"text_clean\"].iloc[i].split()[0], \n",
    "      \" -- idx in vocabulary -->\", \n",
    "      dic_vocabulary[df_train[\"text_clean\"].iloc[i].split()[0]])\n",
    "\n",
    "print(\"vocabulary: \", dict(list(dic_vocabulary.items())[0:5]), \"... (padding element, 0)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = df_test[\"text_clean\"]\n",
    "\n",
    "## create list of n-grams\n",
    "first_corpus = []\n",
    "for string in corpus:\n",
    "    first_words = string.split()\n",
    "    first_grams = [\" \".join(first_words[i:i+1]) for i in range(0, \n",
    "                 len(first_words), 1)]\n",
    "    first_corpus.append(first_grams)\n",
    "    \n",
    "## detect common bigrams and trigrams using the fitted detectors\n",
    "first_corpus = list(bigrams_detector[first_corpus])\n",
    "first_corpus = list(trigrams_detector[first_corpus])\n",
    "\n",
    "## text to sequence with the fitted tokenizer\n",
    "first_text2seq = tokenizer.texts_to_sequences(first_corpus)\n",
    "\n",
    "## padding sequence\n",
    "X_test = kprocessing.sequence.pad_sequences(first_text2seq, maxlen=15,\n",
    "             padding=\"post\", truncating=\"post\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "## start the matrix (length of vocabulary x vector size) with all 0s\n",
    "embeddings = np.zeros((len(dic_vocabulary)+1, 300))\n",
    "for word,idx in dic_vocabulary.items():\n",
    "    ## update the row with vector\n",
    "    try:\n",
    "        embeddings[idx] =  nlp[word]\n",
    "    ## if word not in model then skip and the row stays all 0s\n",
    "    except:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_1\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_2 (InputLayer)           [(None, 15)]         0           []                               \n",
      "                                                                                                  \n",
      " embedding_1 (Embedding)        (None, 15, 300)      141800400   ['input_2[0][0]']                \n",
      "                                                                                                  \n",
      " permute_1 (Permute)            (None, 300, 15)      0           ['embedding_1[0][0]']            \n",
      "                                                                                                  \n",
      " dense_3 (Dense)                (None, 300, 15)      240         ['permute_1[0][0]']              \n",
      "                                                                                                  \n",
      " attention (Permute)            (None, 15, 300)      0           ['dense_3[0][0]']                \n",
      "                                                                                                  \n",
      " multiply_1 (Multiply)          (None, 15, 300)      0           ['embedding_1[0][0]',            \n",
      "                                                                  'attention[0][0]']              \n",
      "                                                                                                  \n",
      " bidirectional_2 (Bidirectional  (None, 15, 30)      37920       ['multiply_1[0][0]']             \n",
      " )                                                                                                \n",
      "                                                                                                  \n",
      " bidirectional_3 (Bidirectional  (None, 30)          5520        ['bidirectional_2[0][0]']        \n",
      " )                                                                                                \n",
      "                                                                                                  \n",
      " dense_4 (Dense)                (None, 64)           1984        ['bidirectional_3[0][0]']        \n",
      "                                                                                                  \n",
      " dense_5 (Dense)                (None, 3)            195         ['dense_4[0][0]']                \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 141,846,259\n",
      "Trainable params: 45,859\n",
      "Non-trainable params: 141,800,400\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "## code attention layer\n",
    "def attention_layer(inputs, neurons):\n",
    "    x = layers.Permute((2,1))(inputs)\n",
    "    x = layers.Dense(neurons, activation=\"softmax\")(x)\n",
    "    x = layers.Permute((2,1), name=\"attention\")(x)\n",
    "    x = layers.multiply([inputs, x])\n",
    "    return x\n",
    "\n",
    "## input\n",
    "x_in = layers.Input(shape=(15,))\n",
    "## embedding\n",
    "x = layers.Embedding(input_dim=embeddings.shape[0],  \n",
    "                     output_dim=embeddings.shape[1], \n",
    "                     weights=[embeddings],\n",
    "                     input_length=15, trainable=False)(x_in)\n",
    "## apply attention\n",
    "x = attention_layer(x, neurons=15)\n",
    "## 2 layers of bidirectional lstm\n",
    "x = layers.Bidirectional(layers.LSTM(units=15, dropout=0.2, \n",
    "                         return_sequences=True))(x)\n",
    "x = layers.Bidirectional(layers.LSTM(units=15, dropout=0.2))(x)\n",
    "## final dense layers\n",
    "x = layers.Dense(64, activation='relu')(x)\n",
    "y_out = layers.Dense(3, activation='softmax')(x)\n",
    "## compile\n",
    "model = models.Model(x_in, y_out)\n",
    "model.compile(loss='sparse_categorical_crossentropy',\n",
    "              optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "# encode y\n",
    "\n",
    "dic_y_mapping = {n:label for n,label in \n",
    "                 enumerate(np.unique(y_train))}\n",
    "inverse_dic = {v:k for k,v in dic_y_mapping.items()}\n",
    "y_train = np.array([inverse_dic[y] for y in y_train])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1255772,)"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1255772, 15)"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import numpy as np\n",
    "\n",
    "#X_train = np.expand_dims(X_train, axis=-1)\n",
    "#this fked up the values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "cannot reshape array of size 18836580 into shape (1255772,1)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_4532/2865944730.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[0mX_train\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mX_train\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m: cannot reshape array of size 18836580 into shape (1255772,1)"
     ]
    }
   ],
   "source": [
    "#import numpy as np\n",
    "\n",
    "X_train = X_train.reshape((X_train.shape[0], 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "y_train is an array of integers\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\win10\\AppData\\Local\\Temp/ipykernel_4532/2130617352.py:1: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  if y_train.dtype == np.int:\n"
     ]
    }
   ],
   "source": [
    "if y_train.dtype == np.int:\n",
    "    print(\"y_train is an array of integers\")\n",
    "else:\n",
    "    print(\"y_train is not an array of integers\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train is an array of integers\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\win10\\AppData\\Local\\Temp/ipykernel_4532/3089006924.py:1: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  if X_train.dtype == np.int:\n"
     ]
    }
   ],
   "source": [
    "if X_train.dtype == np.int:\n",
    "    print(\"X_train is an array of integers\")\n",
    "else:\n",
    "    print(\"X_train is not an array of integers\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "ename": "InvalidArgumentError",
     "evalue": "Graph execution error:\n\nDetected at node 'sparse_categorical_crossentropy/SparseSoftmaxCrossEntropyWithLogits/SparseSoftmaxCrossEntropyWithLogits' defined at (most recent call last):\n    File \"C:\\Users\\win10\\AppData\\Local\\Programs\\Python\\Python39\\lib\\runpy.py\", line 197, in _run_module_as_main\n      return _run_code(code, main_globals, None,\n    File \"C:\\Users\\win10\\AppData\\Local\\Programs\\Python\\Python39\\lib\\runpy.py\", line 87, in _run_code\n      exec(code, run_globals)\n    File \"C:\\Users\\win10\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\ipykernel_launcher.py\", line 16, in <module>\n      app.launch_new_instance()\n    File \"C:\\Users\\win10\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\traitlets\\config\\application.py\", line 846, in launch_instance\n      app.start()\n    File \"C:\\Users\\win10\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\ipykernel\\kernelapp.py\", line 677, in start\n      self.io_loop.start()\n    File \"C:\\Users\\win10\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\tornado\\platform\\asyncio.py\", line 199, in start\n      self.asyncio_loop.run_forever()\n    File \"C:\\Users\\win10\\AppData\\Local\\Programs\\Python\\Python39\\lib\\asyncio\\base_events.py\", line 596, in run_forever\n      self._run_once()\n    File \"C:\\Users\\win10\\AppData\\Local\\Programs\\Python\\Python39\\lib\\asyncio\\base_events.py\", line 1890, in _run_once\n      handle._run()\n    File \"C:\\Users\\win10\\AppData\\Local\\Programs\\Python\\Python39\\lib\\asyncio\\events.py\", line 80, in _run\n      self._context.run(self._callback, *self._args)\n    File \"C:\\Users\\win10\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 457, in dispatch_queue\n      await self.process_one()\n    File \"C:\\Users\\win10\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 446, in process_one\n      await dispatch(*args)\n    File \"C:\\Users\\win10\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 353, in dispatch_shell\n      await result\n    File \"C:\\Users\\win10\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 648, in execute_request\n      reply_content = await reply_content\n    File \"C:\\Users\\win10\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\ipykernel\\ipkernel.py\", line 353, in do_execute\n      res = shell.run_cell(code, store_history=store_history, silent=silent)\n    File \"C:\\Users\\win10\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\ipykernel\\zmqshell.py\", line 533, in run_cell\n      return super(ZMQInteractiveShell, self).run_cell(*args, **kwargs)\n    File \"C:\\Users\\win10\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2901, in run_cell\n      result = self._run_cell(\n    File \"C:\\Users\\win10\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2947, in _run_cell\n      return runner(coro)\n    File \"C:\\Users\\win10\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\IPython\\core\\async_helpers.py\", line 68, in _pseudo_sync_runner\n      coro.send(None)\n    File \"C:\\Users\\win10\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3172, in run_cell_async\n      has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n    File \"C:\\Users\\win10\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3364, in run_ast_nodes\n      if (await self.run_code(code, result,  async_=asy)):\n    File \"C:\\Users\\win10\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3444, in run_code\n      exec(code_obj, self.user_global_ns, self.user_ns)\n    File \"C:\\Users\\win10\\AppData\\Local\\Temp/ipykernel_4532/2442711333.py\", line 2, in <module>\n      training = model.fit(x=X_train, y=y_train, batch_size=256,\n    File \"C:\\Users\\win10\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\keras\\utils\\traceback_utils.py\", line 65, in error_handler\n      return fn(*args, **kwargs)\n    File \"C:\\Users\\win10\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\keras\\engine\\training.py\", line 1564, in fit\n      tmp_logs = self.train_function(iterator)\n    File \"C:\\Users\\win10\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\keras\\engine\\training.py\", line 1160, in train_function\n      return step_function(self, iterator)\n    File \"C:\\Users\\win10\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\keras\\engine\\training.py\", line 1146, in step_function\n      outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"C:\\Users\\win10\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\keras\\engine\\training.py\", line 1135, in run_step\n      outputs = model.train_step(data)\n    File \"C:\\Users\\win10\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\keras\\engine\\training.py\", line 994, in train_step\n      loss = self.compute_loss(x, y, y_pred, sample_weight)\n    File \"C:\\Users\\win10\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\keras\\engine\\training.py\", line 1052, in compute_loss\n      return self.compiled_loss(\n    File \"C:\\Users\\win10\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\keras\\engine\\compile_utils.py\", line 265, in __call__\n      loss_value = loss_obj(y_t, y_p, sample_weight=sw)\n    File \"C:\\Users\\win10\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\keras\\losses.py\", line 152, in __call__\n      losses = call_fn(y_true, y_pred)\n    File \"C:\\Users\\win10\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\keras\\losses.py\", line 272, in call\n      return ag_fn(y_true, y_pred, **self._fn_kwargs)\n    File \"C:\\Users\\win10\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\keras\\losses.py\", line 2084, in sparse_categorical_crossentropy\n      return backend.sparse_categorical_crossentropy(\n    File \"C:\\Users\\win10\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\keras\\backend.py\", line 5630, in sparse_categorical_crossentropy\n      res = tf.nn.sparse_softmax_cross_entropy_with_logits(\nNode: 'sparse_categorical_crossentropy/SparseSoftmaxCrossEntropyWithLogits/SparseSoftmaxCrossEntropyWithLogits'\nReceived a label value of 15 which is outside the valid range of [0, 3).  Label values: 2 0 15 10 8 11 10 11 11 11 11 11 8 9 10 11 1 8 8 8 9 10 10 11 10 8 11 8 11 11 11 11 9 1 8 10 11 7 10 11 11 11 10 9 10 11 10 9 8 15 8 3 10 2 10 10 8 8 11 10 10 10 11 10 12 7 3 9 14 0 8 9 2 2 3 10 11 2 3 10 11 3 1 3 10 11 3 8 10 10 3 8 3 10 8 8 1 10 3 3 3 11 3 7 10 7 11 11 3 15 7 3 15 3 11 10 11 11 15 9 11 3 1 15 8 1 3 11 9 10 1 6 11 9 10 1 11 11 11 3 11 8 3 15 11 11 8 10 10 11 1 14 11 10 9 11 10 0 11 11 8 2 14 10 7 3 11 10 2 8 3 11 2 8 9 10 7 7 10 11 15 8 11 11 8 8 9 3 8 9 11 8 11 8 3 11 11 0 14 11 15 11 11 8 11 10 3 3 11 7 1 11 8 8 14 10 8 9 9 10 10 8 10 11 3 3 11 10 3 1 3 10 8 3 11 1 7 7 1 11 11 2 11 15 9 0 10 1 9 3 8 11 14 11 3 11\n\t [[{{node sparse_categorical_crossentropy/SparseSoftmaxCrossEntropyWithLogits/SparseSoftmaxCrossEntropyWithLogits}}]] [Op:__inference_train_function_13358]",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mInvalidArgumentError\u001b[0m                      Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_4532/2442711333.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m## train\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m training = model.fit(x=X_train, y=y_train, batch_size=256, \n\u001b[0m\u001b[0;32m      3\u001b[0m                      \u001b[0mepochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m10\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mshuffle\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m                      validation_split=0.3)\n\u001b[0;32m      5\u001b[0m \u001b[1;31m## plot loss and accuracy\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\keras\\utils\\traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     68\u001b[0m             \u001b[1;31m# To get the full stack trace, call:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     69\u001b[0m             \u001b[1;31m# `tf.debugging.disable_traceback_filtering()`\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 70\u001b[1;33m             \u001b[1;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfiltered_tb\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     71\u001b[0m         \u001b[1;32mfinally\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     72\u001b[0m             \u001b[1;32mdel\u001b[0m \u001b[0mfiltered_tb\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\tensorflow\\python\\eager\\execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[1;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[0;32m     52\u001b[0m   \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     53\u001b[0m     \u001b[0mctx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 54\u001b[1;33m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0m\u001b[0;32m     55\u001b[0m                                         inputs, attrs, num_outputs)\n\u001b[0;32m     56\u001b[0m   \u001b[1;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mInvalidArgumentError\u001b[0m: Graph execution error:\n\nDetected at node 'sparse_categorical_crossentropy/SparseSoftmaxCrossEntropyWithLogits/SparseSoftmaxCrossEntropyWithLogits' defined at (most recent call last):\n    File \"C:\\Users\\win10\\AppData\\Local\\Programs\\Python\\Python39\\lib\\runpy.py\", line 197, in _run_module_as_main\n      return _run_code(code, main_globals, None,\n    File \"C:\\Users\\win10\\AppData\\Local\\Programs\\Python\\Python39\\lib\\runpy.py\", line 87, in _run_code\n      exec(code, run_globals)\n    File \"C:\\Users\\win10\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\ipykernel_launcher.py\", line 16, in <module>\n      app.launch_new_instance()\n    File \"C:\\Users\\win10\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\traitlets\\config\\application.py\", line 846, in launch_instance\n      app.start()\n    File \"C:\\Users\\win10\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\ipykernel\\kernelapp.py\", line 677, in start\n      self.io_loop.start()\n    File \"C:\\Users\\win10\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\tornado\\platform\\asyncio.py\", line 199, in start\n      self.asyncio_loop.run_forever()\n    File \"C:\\Users\\win10\\AppData\\Local\\Programs\\Python\\Python39\\lib\\asyncio\\base_events.py\", line 596, in run_forever\n      self._run_once()\n    File \"C:\\Users\\win10\\AppData\\Local\\Programs\\Python\\Python39\\lib\\asyncio\\base_events.py\", line 1890, in _run_once\n      handle._run()\n    File \"C:\\Users\\win10\\AppData\\Local\\Programs\\Python\\Python39\\lib\\asyncio\\events.py\", line 80, in _run\n      self._context.run(self._callback, *self._args)\n    File \"C:\\Users\\win10\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 457, in dispatch_queue\n      await self.process_one()\n    File \"C:\\Users\\win10\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 446, in process_one\n      await dispatch(*args)\n    File \"C:\\Users\\win10\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 353, in dispatch_shell\n      await result\n    File \"C:\\Users\\win10\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 648, in execute_request\n      reply_content = await reply_content\n    File \"C:\\Users\\win10\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\ipykernel\\ipkernel.py\", line 353, in do_execute\n      res = shell.run_cell(code, store_history=store_history, silent=silent)\n    File \"C:\\Users\\win10\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\ipykernel\\zmqshell.py\", line 533, in run_cell\n      return super(ZMQInteractiveShell, self).run_cell(*args, **kwargs)\n    File \"C:\\Users\\win10\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2901, in run_cell\n      result = self._run_cell(\n    File \"C:\\Users\\win10\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2947, in _run_cell\n      return runner(coro)\n    File \"C:\\Users\\win10\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\IPython\\core\\async_helpers.py\", line 68, in _pseudo_sync_runner\n      coro.send(None)\n    File \"C:\\Users\\win10\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3172, in run_cell_async\n      has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n    File \"C:\\Users\\win10\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3364, in run_ast_nodes\n      if (await self.run_code(code, result,  async_=asy)):\n    File \"C:\\Users\\win10\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3444, in run_code\n      exec(code_obj, self.user_global_ns, self.user_ns)\n    File \"C:\\Users\\win10\\AppData\\Local\\Temp/ipykernel_4532/2442711333.py\", line 2, in <module>\n      training = model.fit(x=X_train, y=y_train, batch_size=256,\n    File \"C:\\Users\\win10\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\keras\\utils\\traceback_utils.py\", line 65, in error_handler\n      return fn(*args, **kwargs)\n    File \"C:\\Users\\win10\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\keras\\engine\\training.py\", line 1564, in fit\n      tmp_logs = self.train_function(iterator)\n    File \"C:\\Users\\win10\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\keras\\engine\\training.py\", line 1160, in train_function\n      return step_function(self, iterator)\n    File \"C:\\Users\\win10\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\keras\\engine\\training.py\", line 1146, in step_function\n      outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"C:\\Users\\win10\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\keras\\engine\\training.py\", line 1135, in run_step\n      outputs = model.train_step(data)\n    File \"C:\\Users\\win10\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\keras\\engine\\training.py\", line 994, in train_step\n      loss = self.compute_loss(x, y, y_pred, sample_weight)\n    File \"C:\\Users\\win10\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\keras\\engine\\training.py\", line 1052, in compute_loss\n      return self.compiled_loss(\n    File \"C:\\Users\\win10\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\keras\\engine\\compile_utils.py\", line 265, in __call__\n      loss_value = loss_obj(y_t, y_p, sample_weight=sw)\n    File \"C:\\Users\\win10\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\keras\\losses.py\", line 152, in __call__\n      losses = call_fn(y_true, y_pred)\n    File \"C:\\Users\\win10\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\keras\\losses.py\", line 272, in call\n      return ag_fn(y_true, y_pred, **self._fn_kwargs)\n    File \"C:\\Users\\win10\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\keras\\losses.py\", line 2084, in sparse_categorical_crossentropy\n      return backend.sparse_categorical_crossentropy(\n    File \"C:\\Users\\win10\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\keras\\backend.py\", line 5630, in sparse_categorical_crossentropy\n      res = tf.nn.sparse_softmax_cross_entropy_with_logits(\nNode: 'sparse_categorical_crossentropy/SparseSoftmaxCrossEntropyWithLogits/SparseSoftmaxCrossEntropyWithLogits'\nReceived a label value of 15 which is outside the valid range of [0, 3).  Label values: 2 0 15 10 8 11 10 11 11 11 11 11 8 9 10 11 1 8 8 8 9 10 10 11 10 8 11 8 11 11 11 11 9 1 8 10 11 7 10 11 11 11 10 9 10 11 10 9 8 15 8 3 10 2 10 10 8 8 11 10 10 10 11 10 12 7 3 9 14 0 8 9 2 2 3 10 11 2 3 10 11 3 1 3 10 11 3 8 10 10 3 8 3 10 8 8 1 10 3 3 3 11 3 7 10 7 11 11 3 15 7 3 15 3 11 10 11 11 15 9 11 3 1 15 8 1 3 11 9 10 1 6 11 9 10 1 11 11 11 3 11 8 3 15 11 11 8 10 10 11 1 14 11 10 9 11 10 0 11 11 8 2 14 10 7 3 11 10 2 8 3 11 2 8 9 10 7 7 10 11 15 8 11 11 8 8 9 3 8 9 11 8 11 8 3 11 11 0 14 11 15 11 11 8 11 10 3 3 11 7 1 11 8 8 14 10 8 9 9 10 10 8 10 11 3 3 11 10 3 1 3 10 8 3 11 1 7 7 1 11 11 2 11 15 9 0 10 1 9 3 8 11 14 11 3 11\n\t [[{{node sparse_categorical_crossentropy/SparseSoftmaxCrossEntropyWithLogits/SparseSoftmaxCrossEntropyWithLogits}}]] [Op:__inference_train_function_13358]"
     ]
    }
   ],
   "source": [
    "## train\n",
    "training = model.fit(x=X_train, y=y_train, batch_size=256, \n",
    "                     epochs=10, shuffle=True, verbose=0, \n",
    "                     validation_split=0.3)\n",
    "## plot loss and accuracy\n",
    "metrics = [k for k in training.history.keys() if (\"loss\" not in k) and (\"val\" not in k)]\n",
    "fig, ax = plt.subplots(nrows=1, ncols=2, sharey=True)\n",
    "ax[0].set(title=\"Training\")\n",
    "ax11 = ax[0].twinx()\n",
    "ax[0].plot(training.history['loss'], color='black')\n",
    "ax[0].set_xlabel('Epochs')\n",
    "ax[0].set_ylabel('Loss', color='black')\n",
    "for metric in metrics:\n",
    "    ax11.plot(training.history[metric], label=metric)\n",
    "ax11.set_ylabel(\"Score\", color='steelblue')\n",
    "ax11.legend()\n",
    "ax[1].set(title=\"Validation\")\n",
    "ax22 = ax[1].twinx()\n",
    "ax[1].plot(training.history['val_loss'], color='black')\n",
    "ax[1].set_xlabel('Epochs')\n",
    "ax[1].set_ylabel('Loss', color='black')\n",
    "for metric in metrics:\n",
    "     ax22.plot(training.history['val_'+metric], label=metric)\n",
    "ax22.set_ylabel(\"Score\", color=\"steelblue\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## test\n",
    "predicted_prob = model.predict(X_test)\n",
    "predicted = [dic_y_mapping[np.argmax(pred)] for pred in \n",
    "             predicted_prob]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## select observation\n",
    "i = 0\n",
    "txt_instance = df_test[\"text\"].iloc[i]\n",
    "## check true value and predicted value\n",
    "print(\"True:\", y_test[i], \"--> Pred:\", predicted[i], \"| Prob:\", round(np.max(predicted_prob[i]),2))\n",
    "\n",
    "## show explanation\n",
    "### 1. preprocess input\n",
    "first_corpus = []\n",
    "for string in [re.sub(r'[^\\w\\s]','', txt_instance.lower().strip())]:\n",
    "    first_words = string.split()\n",
    "    first_grams = [\" \".join(first_words[i:i+1]) for i in range(0, \n",
    "                 len(first_words), 1)]\n",
    "    first_corpus.append(first_grams)\n",
    "first_corpus = list(bigrams_detector[first_corpus])\n",
    "first_corpus = list(trigrams_detector[first_corpus])\n",
    "X_instance = kprocessing.sequence.pad_sequences(\n",
    "              tokenizer.texts_to_sequences(corpus), maxlen=15, \n",
    "              padding=\"post\", truncating=\"post\")\n",
    "### 2. get attention weights\n",
    "layer = [layer for layer in model.layers if \"attention\" in \n",
    "         layer.name][0]\n",
    "func = K.function([model.input], [layer.output])\n",
    "weights = func(X_instance)[0]\n",
    "weights = np.mean(weights, axis=2).flatten()\n",
    "### 3. rescale weights, remove null vector, map word-weight\n",
    "weights = preprocessing.MinMaxScaler(feature_range=(0,1)).fit_transform(np.array(weights).reshape(-1,1)).reshape(-1)\n",
    "weights = [weights[n] for n,idx in enumerate(X_instance[0]) if idx \n",
    "           != 0]\n",
    "dic_word_weigth = {word:weights[n] for n,word in \n",
    "                   enumerate(first_corpus[0]) if word in \n",
    "                   tokenizer.word_index.keys()}\n",
    "### 4. barplot\n",
    "if len(dic_word_weigth) > 0:\n",
    "   df = pd.DataFrame.from_dict(dic_word_weigth, orient='index', \n",
    "                                columns=[\"score\"])\n",
    "   df.sort_values(by=\"score\", \n",
    "           ascending=True).tail(3).plot(kind=\"barh\", \n",
    "           legend=False).grid(axis='x')\n",
    "   plt.show()\n",
    "else:\n",
    "   print(\"--- No word recognized ---\")\n",
    "### 5. produce html visualization\n",
    "text = []\n",
    "for word in first_corpus[0]:\n",
    "    weight = dic_word_weigth.get(word)\n",
    "    if weight is not None:\n",
    "         text.append('<b><span style=\"background-color:rgba(100,149,237,' + str(weight) + ');\">' + word + '</span></b>')\n",
    "    else:\n",
    "         text.append(word)\n",
    "text = ' '.join(text)\n",
    "### 6. visualize on notebook\n",
    "print(\"\\033[1m\"+\"Text with highlighted words\")\n",
    "from IPython.core.display import display, HTML\n",
    "display(HTML(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
